{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:14:53.900351Z",
     "start_time": "2024-07-31T02:14:43.598539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters gpt4all arxiv"
   ],
   "id": "e280a2c077a29883",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T04:26:08.979558Z",
     "start_time": "2024-07-31T04:26:01.899516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key='')\n",
    "\n",
    "response = tavily.search(query=\"Where does Messi play right now?\", max_results=3)\n",
    "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
    "\n",
    "# You can easily get search result context based on any max tokens straight into your RAG.\n",
    "# The response is a string of the context within the max_token limit.\n",
    "\n",
    "response_context = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
    "\n",
    "# You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
    "# You can use it for baseline\n",
    "response_qna = tavily.qna_search(query=\"Where does Messi play right now?\")\n"
   ],
   "id": "9fa50b634d4ac546",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. 예제 코드 이해하고 실행",
   "id": "d0da076ccf1c3e80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T04:54:41.087241Z",
     "start_time": "2024-07-31T04:54:41.085170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n"
   ],
   "id": "7c3c4bb076758f9f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. 그래프 스테이트 및 노드, 엣지 아래처럼 변경",
   "id": "a6ef08255a1a725f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T06:56:28.969616Z",
     "start_time": "2024-07-31T06:56:26.268117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# index\n",
    "\n",
    "import bs4\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(loader.load())\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "vector_store_retrievers = [vectorstore.as_retriever(), vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    "), vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    "), vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    "), vectorstore.as_retriever(search_kwargs={'k': 1}), vectorstore.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title': 'GPT-4 Technical Report'}}\n",
    ")]"
   ],
   "id": "ff1f27b200346d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2731, which is longer than the specified 1000\n",
      "Created a chunk of size 1538, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 2352, which is longer than the specified 1000\n",
      "Created a chunk of size 1953, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1475, which is longer than the specified 1000\n",
      "Created a chunk of size 2881, which is longer than the specified 1000\n",
      "Created a chunk of size 1980, which is longer than the specified 1000\n",
      "Created a chunk of size 4145, which is longer than the specified 1000\n",
      "Created a chunk of size 2159, which is longer than the specified 1000\n",
      "Created a chunk of size 1317, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Created a chunk of size 1043, which is longer than the specified 1000\n",
      "Created a chunk of size 1578, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n",
      "Created a chunk of size 1464, which is longer than the specified 1000\n",
      "Created a chunk of size 1756, which is longer than the specified 1000\n",
      "Created a chunk of size 1743, which is longer than the specified 1000\n",
      "Created a chunk of size 2407, which is longer than the specified 1000\n",
      "Created a chunk of size 1682, which is longer than the specified 1000\n",
      "Created a chunk of size 1014, which is longer than the specified 1000\n",
      "Created a chunk of size 1036, which is longer than the specified 1000\n",
      "Created a chunk of size 1214, which is longer than the specified 1000\n",
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1986, which is longer than the specified 1000\n",
      "Created a chunk of size 1084, which is longer than the specified 1000\n",
      "Created a chunk of size 1278, which is longer than the specified 1000\n",
      "Created a chunk of size 1462, which is longer than the specified 1000\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:37:57.619472Z",
     "start_time": "2024-07-31T07:37:57.617058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Relevance Checker\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "relevant_prompt = PromptTemplate(\n",
    "    template=\"\"\"Determine if the query is relevant to docs.\n",
    "     A answer should follow the following pattern:\n",
    "    <'relevant': True or False>\n",
    "\n",
    "    {format_instructions}\n",
    "    {question}\n",
    "    {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "relevant_chain = relevant_prompt | llm | parser\n"
   ],
   "id": "1fd81b4b3db646a",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:43:19.399094Z",
     "start_time": "2024-07-31T07:43:17.969498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "\n",
    "generate_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise \n",
    "    \n",
    "    {format_instructions}\n",
    "    {question}\n",
    "    {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llm | parser\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "ans = generate_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(ans[\"answer\"])"
   ],
   "id": "69ed03224ef0a3cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent's memory stream is a long-term memory module that records a comprehensive list of agents' experience in natural language.\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:48:05.331801Z",
     "start_time": "2024-07-31T07:48:05.329162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hallucination Checker\n",
    "\n",
    "hallucination_checker_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    answer: {answer}, question: {question}, context: {context}\n",
    "    \n",
    "    Determine if the answer contains hallucination based on the context and the question.\n",
    "    \n",
    "    \n",
    "    Your answer must follow the following pattern:\n",
    "    <'hallucination': True or False>\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"answer\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "hallucination_chain = hallucination_checker_prompt | llm | parser\n"
   ],
   "id": "6fb5c0a30439fe39",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:58:49.884871Z",
     "start_time": "2024-07-31T07:58:49.877407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    has_hallucination: bool\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def docs_retrieval(state):\n",
    "    print(\"---DOCS Retrieval---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    documents = []\n",
    "    for r in vector_store_retrievers:\n",
    "        documents.append(r.invoke(question))\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def relevant_docs_checker(state):\n",
    "    print(\"---CHECK RELEVANT DOCS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    relevant_documents = []\n",
    "    for doc in documents:\n",
    "        res = relevant_chain.invoke({\"question\": question, \"context\": doc})\n",
    "        if res.get('relevant', False):\n",
    "            if not relevant_checker(doc):\n",
    "                relevant_documents.append(doc)\n",
    "\n",
    "    return {\n",
    "        \"documents\": relevant_documents, \n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "def relevant_checker(possible_docs):\n",
    "    irrelevant_question = 'I like an apple'\n",
    "\n",
    "    res = relevant_chain.invoke({\"question\": irrelevant_question, \"context\": possible_docs})\n",
    "    return res.get('relevant', False)\n",
    "\n",
    "def generate_answer(state):\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    relevant_documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    res = generate_chain.invoke({\"context\": relevant_documents[0], \"question\": question})\n",
    "    possible_answer = res.get('answer', 'I like an apple')\n",
    "\n",
    "    return {\n",
    "        \"documents\": relevant_documents, \n",
    "        \"question\": question, \n",
    "        \"generation\": possible_answer\n",
    "    }\n",
    "\n",
    "def hallucination_checker(state):\n",
    "    print(\"---CHECK HALLUCINATION---\")\n",
    "    question = state[\"question\"]\n",
    "    relevant_documents = state[\"documents\"]\n",
    "    possible_answer = state[\"generation\"]\n",
    "    \n",
    "    has_hallucination = False\n",
    "    for doc in relevant_documents:\n",
    "        res = hallucination_chain.invoke({\"answer\": possible_answer, \"question\": question, \"context\": doc})\n",
    "        has_hallucination |= res.get('hallucination', True)\n",
    "    \n",
    "    return {\n",
    "        \"documents\": relevant_documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": possible_answer,\n",
    "        \"has_hallucination\": has_hallucination\n",
    "    }\n",
    "\n",
    "def web_searcher(state):\n",
    "    print(\"---SEARCH WEB---\")\n",
    "    question = state[\"question\"]\n",
    "    responses = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500, max_results=3)\n",
    "    relevant_documents = [res for res in responses]\n",
    "    \n",
    "    return {\n",
    "        \"relevant_documents\": relevant_documents,\n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "### Edges\n",
    "def decide_to_generate(state):\n",
    "    print(\"---Let System generate or web search---\")\n",
    "    relevant_documents = state[\"documents\"]\n",
    "\n",
    "    if len(relevant_documents) == 0:\n",
    "        return \"web_searcher\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate_answer\"\n",
    "\n",
    "def decide_to_answer(state):\n",
    "    print(\"---Let System print the answer or not---\")\n",
    "    has_hallucination = state[\"has_hallucination\"]\n",
    "    if has_hallucination:\n",
    "        print(\"---DECISION: Re-Generate---\")\n",
    "        return \"web_searcher\"\n",
    "    else:\n",
    "        print(\"---DECISION: Print The Answer---\")\n",
    "        return \"useful\"\n",
    "\n",
    "state_machine = StateGraph(State)\n",
    "\n",
    "# Define the nodes\n",
    "state_machine.add_node(\"docs_retrieval\", docs_retrieval)\n",
    "state_machine.add_node(\"relevant_docs_checker\", relevant_docs_checker)\n",
    "state_machine.add_node(\"generate_answer\", generate_answer)\n",
    "state_machine.add_node(\"hallucination_checker\", hallucination_checker)\n",
    "state_machine.add_node(\"web_searcher\", web_searcher)"
   ],
   "id": "96731373d138a09d",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:58:50.797142Z",
     "start_time": "2024-07-31T07:58:50.794016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build graph\n",
    "state_machine.set_entry_point(\"docs_retrieval\")\n",
    "state_machine.add_edge(\"docs_retrieval\", \"relevant_docs_checker\")\n",
    "state_machine.add_conditional_edges(\n",
    "    \"relevant_docs_checker\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_searcher\": \"web_searcher\",\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "    },\n",
    ")\n",
    "state_machine.add_edge(\"web_searcher\", \"relevant_docs_checker\")\n",
    "state_machine.add_edge(\"generate_answer\", \"hallucination_checker\")\n",
    "state_machine.add_conditional_edges(\n",
    "    \"hallucination_checker\",\n",
    "    decide_to_answer,\n",
    "    {\n",
    "        \"web_searcher\": \"web_searcher\",\n",
    "        \"useful\": END,\n",
    "    },\n",
    ")"
   ],
   "id": "e227261db330c2d2",
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:59:13.464772Z",
     "start_time": "2024-07-31T07:58:51.087325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compile\n",
    "app = state_machine.compile()\n",
    "\n",
    "# Test\n",
    "\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")"
   ],
   "id": "4d8b1961061f2cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DOCS Retrieval---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seokjoongkim/llm/joon.233/myenv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: docs_retrieval:'\n",
      "---CHECK RELEVANT DOCS---\n",
      "---Let System generate or web search---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: relevant_docs_checker:'\n",
      "---GENERATE ANSWER---\n",
      "'Finished running: generate_answer:'\n",
      "---CHECK HALLUCINATION---\n",
      "---Let System print the answer or not---\n",
      "---DECISION: Print The Answer---\n",
      "'Finished running: hallucination_checker:'\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ea08b2d204ba1509"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T07:59:29.408207Z",
     "start_time": "2024-07-31T07:59:29.405021Z"
    }
   },
   "cell_type": "code",
   "source": "value['generation']",
   "id": "7f45272180d1e7b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The types of agent memory mentioned include Sensory memory, Short-term memory, Long-term memory.'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d1bcdfc7a44a050"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
